{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880c8fa6",
   "metadata": {},
   "source": [
    "# üßå Monstrinho 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4a243",
   "metadata": {},
   "source": [
    "O sol amanhece no reino de Lumi e um novo monstro aparece para incomodar. Vamos l√° ent√£o. Para derrotar esse monstro, vamos utilizar 3 novas fun√ß√µes de ativa√ß√£o na rede neural. Realizaremos isso atrav√©s dos novos conceitos aprendidos na aula de MLP (Multi-Layer Perceptron) onde montamos um modelo do zero com Python puro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab436a",
   "metadata": {},
   "source": [
    "A seguir, est√£o as 4 classes (`Valor`, `Neur√¥nio`, `Camada` e `MLP`) j√° criadas na aula de MLP. As mudan√ßas que existem foram feitas por mim para melhorar a legibilidade e a organiza√ß√£o do c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2af54",
   "metadata": {},
   "source": [
    "## üî¢ Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504be265",
   "metadata": {},
   "source": [
    "Esta √© a classe Valor, que representa um valor num√©rico com suporte a diferencia√ß√£o autom√°tica. Ela possui m√©todos para opera√ß√µes matem√°ticas, como adi√ß√£o, subtra√ß√£o, multiplica√ß√£o e exponencia√ß√£o, al√©m de m√©todos para calcular a fun√ß√£o sigmoide e backpropagation. √â not√≥rio lembrar que realizasse os c√°lculos mesmo com ordem invertida, onde o objeto Valor pode ser o primeiro ou segundo operando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59390afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Valor:\n",
    "    \"\"\"\n",
    "    Classe que representa um valor num√©rico com suporte a diferencia√ß√£o autom√°tica.\n",
    "    \n",
    "    Cada inst√¢ncia armazena:\n",
    "      - data: o valor num√©rico.\n",
    "      - progenitor: tupla com os valores dos quais este foi derivado.\n",
    "      - operador_mae: string representando a opera√ß√£o que gerou o valor.\n",
    "      - grad: gradiente (inicialmente zero) usado no backpropagation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, progenitor=(), operador_mae=\"\", rotulo=\"\"):\n",
    "        \"\"\"\n",
    "        Inicializa uma inst√¢ncia de Valor.\n",
    "        \n",
    "        Args:\n",
    "            data (float): o valor num√©rico.\n",
    "            progenitor (tuple): valores anteriores que contribu√≠ram para este.\n",
    "            operador_mae (str): opera√ß√£o que gerou o valor.\n",
    "            rotulo (str): r√≥tulo opcional para identifica√ß√£o.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.progenitor = progenitor\n",
    "        self.operador_mae = operador_mae\n",
    "        self.rotulo = rotulo\n",
    "        self.grad = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Retorna uma representa√ß√£o string simplificada do objeto.\"\"\"\n",
    "        return f\"Valor(data={self.data})\"\n",
    "\n",
    "    def __add__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de adi√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self + outro_valor.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser somado.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a soma.\n",
    "        \"\"\"\n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "            \n",
    "        progenitor = (self, outro_valor)\n",
    "        data = self.data + outro_valor.data\n",
    "        operador_mae = \"+\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_adicao():\n",
    "            self.grad += resultado.grad\n",
    "            outro_valor.grad += resultado.grad\n",
    "            \n",
    "        resultado.propagar = propagar_adicao\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "    def __mul__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de multiplica√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self * outro_valor.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser multiplicado.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando o produto.\n",
    "        \"\"\"\n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "            \n",
    "        progenitor = (self, outro_valor)\n",
    "        data = self.data * outro_valor.data\n",
    "        operador_mae = \"*\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_multiplicacao():\n",
    "            self.grad += resultado.grad * outro_valor.data\n",
    "            outro_valor.grad += resultado.grad * self.data\n",
    "            \n",
    "        resultado.propagar = propagar_multiplicacao\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Calcula a exponencial do valor.\n",
    "        \n",
    "        Realiza a opera√ß√£o: exp(self).\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a exponencial.\n",
    "        \"\"\"\n",
    "        progenitor = (self, )\n",
    "        data = math.exp(self.data)\n",
    "        operador_mae = \"exp\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_exp():\n",
    "            self.grad += resultado.grad * data \n",
    "        \n",
    "        resultado.propagar = propagar_exp\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "    def __pow__(self, expoente):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de exponencia√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self ** expoente.\n",
    "        \n",
    "        Args:\n",
    "            expoente (int ou float): expoente da opera√ß√£o.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a exponencia√ß√£o.\n",
    "        \"\"\"\n",
    "        assert isinstance(expoente, (int, float)), \"Expoente deve ser um n√∫mero.\"\n",
    "        progenitor = (self, )\n",
    "        data = self.data ** expoente\n",
    "        operador_mae = f\"**{expoente}\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_pow():\n",
    "            self.grad += resultado.grad * (expoente * self.data ** (expoente - 1))\n",
    "        \n",
    "        resultado.propagar = propagar_pow\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "    def __truediv__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de divis√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self / outro_valor.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): divisor.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a divis√£o.\n",
    "        \"\"\"\n",
    "        return self * outro_valor ** (-1)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de nega√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: -self.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando o valor negativo.\n",
    "        \"\"\"\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de subtra√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self - outro_valor.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser subtra√≠do.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a subtra√ß√£o.\n",
    "        \"\"\"\n",
    "        return self + (-outro_valor)\n",
    "\n",
    "    def __radd__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de adi√ß√£o reversa.\n",
    "        \n",
    "        Permite opera√ß√µes onde Valor est√° √† direita: outro_valor + self.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser somado.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: resultado da adi√ß√£o.\n",
    "        \"\"\"\n",
    "        return self + outro_valor\n",
    "\n",
    "    def __rmul__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de multiplica√ß√£o reversa.\n",
    "        \n",
    "        Permite opera√ß√µes onde Valor est√° √† direita: outro_valor * self.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser multiplicado.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: resultado da multiplica√ß√£o.\n",
    "        \"\"\"\n",
    "        return self * outro_valor\n",
    "\n",
    "    def sig(self):\n",
    "        \"\"\"\n",
    "        Calcula a fun√ß√£o sigmoide.\n",
    "        \n",
    "        Realiza a opera√ß√£o: exp(self) / (exp(self) + 1).\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando o resultado da sigmoide.\n",
    "        \"\"\"\n",
    "        return self.exp() / (self.exp() + 1)\n",
    "\n",
    "    def propagar(self):\n",
    "        \"\"\"\n",
    "        Fun√ß√£o de propaga√ß√£o (backpropagation) do gradiente.\n",
    "        \n",
    "        Este m√©todo deve ser sobrescrito pelas opera√ß√µes espec√≠ficas.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def propagar_tudo(self):\n",
    "        \"\"\"\n",
    "        Executa o backpropagation atrav√©s de todos os n√≥s (valores) conectados.\n",
    "        \n",
    "        Atribui gradiente 1 ao v√©rtice folha e propaga recursivamente utilizando uma ordem topol√≥gica dos n√≥s.\n",
    "        \"\"\"\n",
    "        self.grad = 1\n",
    "        ordem_topologica = []\n",
    "        visitados = set()\n",
    "\n",
    "        def constroi_ordem_topologica(v):\n",
    "            if v not in visitados:\n",
    "                visitados.add(v)\n",
    "                for progenitor in v.progenitor:\n",
    "                    constroi_ordem_topologica(progenitor)\n",
    "                ordem_topologica.append(v)\n",
    "\n",
    "        constroi_ordem_topologica(self)\n",
    "\n",
    "        for vertice in reversed(ordem_topologica):\n",
    "            vertice.propagar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2b97f",
   "metadata": {},
   "source": [
    "## ü§ì Neur√¥nio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd5247",
   "metadata": {},
   "source": [
    "A classe Neur√¥nio representa um neur√¥nio em uma rede neural. Ela possui pesos e um vi√©s, que s√£o inicializados aleatoriamente. O neur√¥nio calcula a sa√≠da usando a fun√ß√£o sigmoide e realiza o backpropagation para atualizar os pesos e o vi√©s com base no erro da previs√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd0c222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Neuronio:\n",
    "    \"\"\"\n",
    "    Representa um neur√¥nio simples com pesos e vi√©s para uso em uma rede neural.\n",
    "\n",
    "    Este neur√¥nio utiliza a classe Valor para armazenar seus par√¢metros e realizar a\n",
    "    diferencia√ß√£o autom√°tica durante o treinamento.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_dados_entrada, ativacao=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Inicializa um neur√¥nio com pesos e vi√©s aleat√≥rios.\n",
    "        \n",
    "        Args:\n",
    "            num_dados_entrada (int): n√∫mero de entradas do neur√¥nio.\n",
    "            ativacao (str): fun√ß√£o de ativa√ß√£o a ser utilizada. Op√ß√µes: \"sigmoid\", \"relu\", \"tanh\" e \"leaky_relu\".\n",
    "        \"\"\"\n",
    "        self.vies = Valor(random.uniform(-1, 1))\n",
    "\n",
    "        self.pesos = []\n",
    "        for i in range(num_dados_entrada):\n",
    "            self.pesos.append(Valor(random.uniform(-1, 1)))\n",
    "\n",
    "        if ativacao == \"sigmoid\":\n",
    "            self.ativacao = lambda x: x.sig()\n",
    "        elif ativacao == \"relu\":\n",
    "            self.ativacao = lambda x: x.relu()\n",
    "        elif ativacao == \"tanh\":\n",
    "            self.ativacao = lambda x: x.tanh()\n",
    "        elif ativacao == \"leaky_relu\":\n",
    "            self.ativacao = lambda x: x.leaky_relu()\n",
    "        else:\n",
    "            raise ValueError(f\"Fun√ß√£o de ativa√ß√£o '{ativacao}' n√£o suportada.\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Realiza a passagem forward do neur√¥nio.\n",
    "\n",
    "        Calcula a soma ponderada das entradas e aplica a fun√ß√£o sigmoide para\n",
    "        determinar a sa√≠da do neur√¥nio.\n",
    "\n",
    "        Args:\n",
    "            x (list[Valor]): lista de objetos Valor representando as entradas.\n",
    "\n",
    "        Returns:\n",
    "            Valor: objeto Valor representando a sa√≠da do neur√¥nio.\n",
    "        \"\"\"\n",
    "        assert len(x) == len(\n",
    "            self.pesos\n",
    "        ), \"O n√∫mero de entradas deve ser igual ao n√∫mero de pesos.\"\n",
    "\n",
    "        soma = 0\n",
    "        for info_entrada, peso_interno in zip(x, self.pesos):\n",
    "            soma += info_entrada * peso_interno\n",
    "\n",
    "        soma += self.vies\n",
    "\n",
    "        return self.ativacao(soma)\n",
    "\n",
    "    def parametros(self):\n",
    "        \"\"\"\n",
    "        Retorna uma lista com os par√¢metros do neur√¥nio (pesos e vi√©s).\n",
    "\n",
    "        Returns:\n",
    "            list[Valor]: lista contendo os pesos e o vi√©s.\n",
    "        \"\"\"\n",
    "        return self.pesos + [self.vies]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62566653",
   "metadata": {},
   "source": [
    "Uma pequena altera√ß√£o ocorreu na classe `Neuronio` para permitir a utiliza√ß√£o de fun√ß√µes de ativa√ß√£o diferentes. Para isso, foi adicionado um par√¢metro `ativacao` no construtor da classe. Esse par√¢metro pode ser uma fun√ß√£o de ativa√ß√£o como `sigmoide`, `relu`, `tanh` ou `leaky_relu`. O m√©todo `ativar` agora utiliza essa fun√ß√£o de ativa√ß√£o para calcular a sa√≠da do neur√¥nio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab5730",
   "metadata": {},
   "source": [
    "## üéÇ Camada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b70f2",
   "metadata": {},
   "source": [
    "A classe Camada representa uma camada de neur√¥nios em uma rede neural. Ela possui um n√∫mero espec√≠fico de neur√¥nios e √© respons√°vel por calcular a sa√≠da da camada com base nas entradas recebidas. Realiza o forward pass e agrega os parametros de cada neur√¥nio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad7f3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camada:\n",
    "    \"\"\"\n",
    "    Representa uma camada em uma rede neural composta por m√∫ltiplos neur√¥nios.\n",
    "\n",
    "    Cada camada gerencia um conjunto de neur√¥nios, realizando a passagem forward\n",
    "    e agregando os par√¢metros (pesos e vi√©s) de cada neur√¥nio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_neuronios, num_dados_entrada, ativacao=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Inicializa a camada com um n√∫mero espec√≠fico de neur√¥nios, cada um com\n",
    "        um determinado n√∫mero de entradas.\n",
    "\n",
    "        Args:\n",
    "            num_neuronios (int): n√∫mero de neur√¥nios na camada.\n",
    "            num_dados_entrada (int): n√∫mero de entradas para cada neur√¥nio.\n",
    "        \"\"\"\n",
    "        self.neuronios = []\n",
    "        for _ in range(num_neuronios):\n",
    "            neuronio = Neuronio(num_dados_entrada, ativacao=ativacao)\n",
    "            self.neuronios.append(neuronio)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Realiza a passagem forward na camada.\n",
    "\n",
    "        Aplica cada neur√¥nio da camada √† mesma entrada e retorna os dados de sa√≠da.\n",
    "\n",
    "        Args:\n",
    "            x (list[Valor]): lista de objetos Valor representando as entradas da camada.\n",
    "\n",
    "        Returns:\n",
    "            Valor ou list[Valor]: sa√≠da de um √∫nico neur√¥nio se houver apenas um,\n",
    "            ou lista com as sa√≠das de todos os neur√¥nios.\n",
    "        \"\"\"\n",
    "        dados_de_saida = []\n",
    "        for neuronio in self.neuronios:\n",
    "            informacao = neuronio(x)\n",
    "            dados_de_saida.append(informacao)\n",
    "\n",
    "        if len(dados_de_saida) == 1:\n",
    "            return dados_de_saida[0]\n",
    "        else:\n",
    "            return dados_de_saida\n",
    "\n",
    "    def parametros(self):\n",
    "        \"\"\"\n",
    "        Agrega e retorna todos os par√¢metros (pesos e vi√©s) de cada neur√¥nio da camada.\n",
    "\n",
    "        Returns:\n",
    "            list[Valor]: lista contendo todos os par√¢metros dos neur√¥nios da camada.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for neuronio in self.neuronios:\n",
    "            params_neuronio = neuronio.parametros()\n",
    "            params.extend(params_neuronio)\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab290f7",
   "metadata": {},
   "source": [
    "A classe `Camada` tamb√©m foi alterada para permitir a utiliza√ß√£o de fun√ß√µes de ativa√ß√£o diferentes. O construtor agora aceita um par√¢metro `ativacao` que pode ser uma fun√ß√£o de ativa√ß√£o como `sigmoide`, `relu`, `tanh` ou `leaky_relu`. O m√©todo `ativar` utiliza essa fun√ß√£o de ativa√ß√£o para calcular a sa√≠da da camada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1119298",
   "metadata": {},
   "source": [
    "## üß† MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcbbe9",
   "metadata": {},
   "source": [
    "Por fim, nossa √∫ltima classe da nossa rede neural √© a `MLP` (Multi-Layer Perceptron). Ela representa uma rede neural com m√∫ltiplas camadas. A `MLP` organiza as camadas da rede, permitindo a passagem forward dos dados e a agrega√ß√£o dos par√¢metros de todas as camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7a4989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Representa uma rede neural do tipo MLP (Multi-Layer Perceptron).\n",
    "\n",
    "    Essa classe organiza as camadas da rede, permitindo a passagem forward dos dados e\n",
    "    a agrega√ß√£o dos par√¢metros (pesos e vi√©s) de todas as camadas.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_dados_entrada, num_neuronios_por_camada, ativacao=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Inicializa a MLP com um n√∫mero definido de entradas e uma lista que especifica\n",
    "        o n√∫mero de neur√¥nios em cada camada.\n",
    "\n",
    "        Args:\n",
    "            num_dados_entrada (int): n√∫mero de entradas da rede.\n",
    "            num_neuronios_por_camada (list[int]): lista com o n√∫mero de neur√¥nios para cada camada.\n",
    "        \"\"\"\n",
    "        percurso = [num_dados_entrada] + num_neuronios_por_camada\n",
    "\n",
    "        camadas = []\n",
    "        for i in range(len(num_neuronios_por_camada)):\n",
    "            camada = Camada(num_neuronios_por_camada[i], percurso[i], ativacao=ativacao)\n",
    "            camadas.append(camada)\n",
    "\n",
    "        self.camadas = camadas\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Realiza a passagem forward pela rede.\n",
    "\n",
    "        Cada camada processa a entrada e o resultado √© passado para a pr√≥xima camada.\n",
    "\n",
    "        Args:\n",
    "            x (list[Valor] ou Valor): dados de entrada para a rede.\n",
    "\n",
    "        Returns:\n",
    "            Valor ou list[Valor]: sa√≠da final da rede ap√≥s a passagem por todas as camadas.\n",
    "        \"\"\"\n",
    "        for camada in self.camadas:\n",
    "            x = camada(x)\n",
    "        return x\n",
    "\n",
    "    def parametros(self):\n",
    "        \"\"\"\n",
    "        Agrega e retorna todos os par√¢metros (pesos e vi√©s) de todas as camadas da rede.\n",
    "\n",
    "        Returns:\n",
    "            list[Valor]: lista contendo os par√¢metros de cada camada.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for camada in self.camadas:\n",
    "            parametros_camada = camada.parametros()\n",
    "            params.extend(parametros_camada)\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b3795",
   "metadata": {},
   "source": [
    "Por fim, a classe `MLP` tamb√©m foi alterada para permitir a utiliza√ß√£o de fun√ß√µes de ativa√ß√£o diferentes. O construtor agora aceita um par√¢metro `ativacao` que pode ser uma fun√ß√£o de ativa√ß√£o como `sigmoide`, `relu`, `tanh` ou `leaky_relu`. O m√©todo `ativar` utiliza essa fun√ß√£o de ativa√ß√£o para calcular a sa√≠da da rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68c58c",
   "metadata": {},
   "source": [
    "## ü™ù Fun√ß√µes de Ativa√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8bc988",
   "metadata": {},
   "source": [
    "Show! Agora que definimos todas as classes que usaremos para essa atividade, vamos abordar as 3 novas fun√ß√µes de ativa√ß√£o que vamos usar para treinar nossa rede neural. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2ff3e",
   "metadata": {},
   "source": [
    "### 0Ô∏è‚É£ Sigmoide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec8938",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Talvez seja legal colocar aqui uma explica√ß√£o tamb√©m da fun√ß√£o sigmoide</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9fc78",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127eec73",
   "metadata": {},
   "source": [
    "A ReLU √© uma das fun√ß√µes de ativa√ß√£o mais populares para redes neurais artificiais, e encontra aplica√ß√£o em vis√£o computacional e reconhecimento de fala usando redes neurais profundas e neuroci√™ncia computacional. [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ca5f5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fd7b3",
   "metadata": {},
   "source": [
    "Uma das principais caracter√≠sticas da fun√ß√£o de ativa√ß√£o ReLU s√£o:\n",
    "\n",
    "- Simples e eficiente\n",
    "- Resolve parcialmente o problema do **desvanecimento do gradiente**.\n",
    "- Pode \"morrer\" para entradas negativas (gradiente zero).\n",
    "\n",
    "Quando dizemos que a ReLU √© simples e eficiente, nos referimos ao fato de que ela √© computacionalmente barata, pois envolve apenas uma compara√ß√£o e n√£o requer opera√ß√µes exponenciais ou trigonom√©tricas. Isso a torna adequada para redes neurais profundas, onde a efici√™ncia computacional √© crucial. J√° o problema do desvanecimento do gradiente ocorre quando os gradientes se tornam muito pequenos durante o treinamento, dificultando a atualiza√ß√£o dos pesos. A ReLU ajuda a mitigar esse problema, permitindo que os gradientes fluam mais facilmente atrav√©s das camadas da rede. No entanto, a ReLU pode \"morrer\" para entradas negativas, resultando em neur√¥nios que n√£o se ativam e n√£o contribuem para o aprendizado. Isso pode ocorrer quando os pesos s√£o atualizados de forma que a sa√≠da do neur√¥nio permane√ßa negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cfef0",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/ReLU.jpg\" alt=\"ReLU Function\" width=\"300\"/>\n",
    "<p>Legenda 1: Esta √© a fun√ß√£o ReLU. Ela √© linear para valores positivos e zero para valores negativos.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c54ce",
   "metadata": {},
   "source": [
    "Comparando com a fun√ß√£o de ativa√ß√£o sigmoide que mapeia para o intervalo (0, 1), o que pode gerar gradientes pequenos em extremos. Mas, a ReLU j√° √© mais eficiente e n√£o satura no lado positivo, o que significa que os gradientes permanecem grandes e n√£o se tornam pequenos, permitindo que a rede aprenda mais rapidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acfa2f",
   "metadata": {},
   "source": [
    "Vamos aproveitar, e j√° criar a fun√ß√£o em Python para a ReLU. A fun√ß√£o `relu` recebe um valor e retorna o valor m√°ximo entre 0 e o valor de entrada. Se o valor for menor que 0, a fun√ß√£o retorna 0. Caso contr√°rio, retorna o pr√≥prio valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36703c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(self: Valor) -> Valor:\n",
    "    \"\"\"\n",
    "    Aplica a fun√ß√£o ReLU (Rectified Linear Unit) ao valor. A fun√ß√£o ReLU retorna o valor se ele for positivo, caso contr√°rio retorna 0. Realiza a opera√ß√£o: max(0, self).\n",
    "    \n",
    "    Args:\n",
    "        Valor (self): valor a ser processado.\n",
    "        \n",
    "    Returns:\n",
    "        Valor: novo objeto representando o resultado da fun√ß√£o ReLU.\n",
    "    \"\"\"\n",
    "    x = self\n",
    "    data = x.data if x.data > 0 else 0.0\n",
    "    resultado = Valor(data, (x,), \"ReLU\")\n",
    "\n",
    "    def propagar_relu():\n",
    "        x.grad += resultado.grad * (1.0 if x.data > 0 else 0.0)\n",
    "    \n",
    "    resultado.propagar = propagar_relu\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3eb49d",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Tanh (Tangente Hiperb√≥lica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1106c",
   "metadata": {},
   "source": [
    "A fun√ß√£o tanh produz valores no intervalo de -1 a +1. Isso significa que ele pode lidar com valores negativos de forma mais eficaz do que a fun√ß√£o sigm√≥ide, que tem um intervalo de 0 a 1. [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9de0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tanh(x) = \\frac{(e^x - e^{-x})}{(e^x + e^{-x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9de24",
   "metadata": {},
   "source": [
    "As principais caracter√≠sticas que podemos destacar da fun√ß√£o tanh s√£o:\n",
    "\n",
    "- Sa√≠da entre (-1, 1), centrada em zero.\n",
    "- Derivada maior que a da sigmoide, o que acelera o aprendizado.\n",
    "\n",
    "A sa√≠da entre (-1, 1) significa que a fun√ß√£o tanh pode lidar com valores negativos de forma mais eficaz do que a fun√ß√£o sigmoide, que tem um intervalo de 0 a 1. Isso pode ser √∫til em redes neurais profundas, onde a normaliza√ß√£o dos dados pode ser importante. A derivada da fun√ß√£o tanh √© maior do que a da sigmoide, o que significa que os gradientes s√£o mais fortes e podem acelerar o aprendizado. Isso pode levar a uma converg√™ncia mais r√°pida durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75d391",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/Tanh.jpg\" alt=\"Tanh Function\" width=\"300\"/>\n",
    "<p>Legenda 2: Esta j√° √© a fun√ß√£o tanh. Ela √© linear para valores pr√≥ximos de zero e n√£o linear para valores extremos.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28acad",
   "metadata": {},
   "source": [
    "Comparando com a fun√ß√£o sigmoide, a tanh √© mais adequada para entradas centradas em zero. Isso significa que a fun√ß√£o tanh pode lidar melhor com dados que t√™m uma m√©dia pr√≥xima de zero, o que pode ser √∫til em muitas aplica√ß√µes de aprendizado de m√°quina. Entretanto, a tanh ainda sofre com o problema do desvanecimento de gradiente em extremos, onde os gradientes se tornam muito pequenos e dificultam o aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408bfb2",
   "metadata": {},
   "source": [
    "Bom, agora vamos criar a fun√ß√£o em Python para a tanh. A fun√ß√£o `tanh` recebe um valor e retorna o valor da tangente hiperb√≥lica do valor de entrada. Se o valor for menor que 0, a fun√ß√£o retorna -1. Caso contr√°rio, retorna o pr√≥prio valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5b5fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(self: Valor) -> Valor:\n",
    "    \"\"\"\n",
    "    Aplica a fun√ß√£o tangente hiperb√≥lica (tanh) ao valor. A fun√ß√£o tanh retorna o valor normalizado entre -1 e 1. Realiza a opera√ß√£o: tanh(self).\n",
    "\n",
    "    Args:\n",
    "        Valor (self): valor a ser processado.\n",
    "\n",
    "    Returns:\n",
    "        Valor: novo objeto representando o resultado da fun√ß√£o tanh.\n",
    "    \"\"\"\n",
    "    x = self\n",
    "    e_pos = (x * 2).exp()\n",
    "    t = (e_pos - 1) / (e_pos + 1)\n",
    "    resultado = Valor(t.data, (x,), \"tanh\")\n",
    "\n",
    "    def propagar_tanh():\n",
    "        x.grad += resultado.grad * (1 - t.data**2)\n",
    "\n",
    "    resultado.propagar = propagar_tanh\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ff120",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfade8",
   "metadata": {},
   "source": [
    "Leaky ReLU √© uma vers√£o melhorada da fun√ß√£o ReLU para resolver o problema de \"morte\" da ReLU, pois tem uma pequena inclina√ß√£o positiva na √°rea negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074cddd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Leaky ReLU}(x) = \n",
    "\\begin{cases}\n",
    "x & \\text{se } x > 0 \\\\\n",
    "\\alpha x & \\text{caso contr√°rio}\n",
    "\\end{cases}\n",
    "\\quad \\text{(tipicamente } \\alpha = 0.01\\text{)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94440b8a",
   "metadata": {},
   "source": [
    "As principais caracter√≠sticas que podemos destacar da fun√ß√£o Leaky ReLU s√£o:\n",
    "\n",
    "- Variante do ReLU que permite gradiente pequeno quando $ x < 0 $.\n",
    "- Reduz o risco de \"neur√¥nios mortos\".\n",
    "\n",
    "A possibilidade de permitir um pequeno gradiente negativo quando $ x < 0 $ acontece devido √† inclina√ß√£o pequena, o que significa que mesmo quando a entrada √© negativa, a fun√ß√£o ainda tem um pequeno gradiente positivo. Isso ajuda a evitar o problema de \"neur√¥nios mortos\", onde os neur√¥nios n√£o se ativam e n√£o contribuem para o aprendizado. A Leaky ReLU √© uma fun√ß√£o de ativa√ß√£o mais robusta do que a ReLU padr√£o, pois pode lidar melhor com entradas negativas e ainda mant√©m as vantagens da ReLU em termos de efici√™ncia computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd22da",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/Leaky ReLU.jpg\" alt=\"Tanh Function\" width=\"300\"/>\n",
    "<p>Legenda 3: Por fim, temos a Leaky ReLU. Ela √© linear para valores positivos e tem uma pequena inclina√ß√£o negativa para valores negativos.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bb9db",
   "metadata": {},
   "source": [
    "Realizando uma compara√ß√£o com a fun√ß√£o sigmoide, a Leaky ReLU √© mais eficiente em termos computacionais, pois n√£o envolve opera√ß√µes exponenciais ou trigonom√©tricas. Isso a torna adequada para redes neurais profundas, onde a efici√™ncia computacional √© crucial. Al√©m disso, a Leaky ReLU n√£o sofre com o problema do desvanecimento do gradiente da mesma forma que a sigmoide e a tanh, permitindo que os gradientes fluam mais facilmente atrav√©s das camadas da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456428b",
   "metadata": {},
   "source": [
    "Para terminar, vamos criar a fun√ß√£o em Python para a Leaky ReLU. A fun√ß√£o `leaky_relu` recebe um valor e retorna o valor m√°ximo entre 0 e o valor de entrada. Se o valor for menor que 0, a fun√ß√£o retorna 0.01 vezes o valor de entrada. Caso contr√°rio, retorna o pr√≥prio valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7aaab39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(self, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Aplica a fun√ß√£o Leaky ReLU ao valor. A fun√ß√£o Leaky ReLU retorna o valor se ele for positivo, caso contr√°rio retorna alpha * valor. Realiza a opera√ß√£o: max(0, self) + alpha * min(0, self).\n",
    "\n",
    "    Args:\n",
    "        Valor (self): valor a ser processado.\n",
    "        alpha (float): coeficiente de inclina√ß√£o para valores negativos.\n",
    "\n",
    "    Returns:\n",
    "        Valor: novo objeto representando o resultado da fun√ß√£o Leaky ReLU.\n",
    "    \"\"\"\n",
    "    x = self\n",
    "    data = x.data if x.data > 0 else alpha * x.data\n",
    "    resultado = Valor(data, (x,), \"LeakyReLU\")\n",
    "\n",
    "    def propagar_leaky():\n",
    "        x.grad += resultado.grad * (1.0 if x.data > 0 else alpha)\n",
    "\n",
    "    resultado.propagar = propagar_leaky\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb0dce",
   "metadata": {},
   "source": [
    "### üß™ Testando as fun√ß√µes de ativa√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776a123",
   "metadata": {},
   "source": [
    "Primeiro, para testarmos essas fun√ß√µes de ativa√ß√£o, precisamos instanciar a classe `Valor` e criar um valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a898c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valor.relu = relu\n",
    "Valor.tanh = tanh\n",
    "Valor.leaky_relu = leaky_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c64f3",
   "metadata": {},
   "source": [
    "Agora que definimos na classe `Valor` as fun√ß√µes de ativa√ß√£o que definimos anteriormente, vamos utiliz√°-las com o mesmo exemplo que o professor usou na aula de MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3525d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "y_true = [1, 0, 0.2, 0.5]\n",
    "\n",
    "NUM_DADOS_DE_ENTRADA = 3  \n",
    "NUM_DADOS_DE_SAIDA = 1    \n",
    "CAMADAS_OCULTAS = [3, 2]  \n",
    "\n",
    "arquitetura_da_rede = CAMADAS_OCULTAS + [NUM_DADOS_DE_SAIDA]\n",
    "\n",
    "minha_mlp = MLP(NUM_DADOS_DE_ENTRADA, arquitetura_da_rede, ativacao=\"leaky_relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c65d6cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 0 Loss: 0.2977623725524472\n",
      "Epoca: 20 Loss: 1.3407764669030975\n",
      "Epoca: 40 Loss: 1.327089663070773\n",
      "Epoca: 60 Loss: 1.3139596176021158\n",
      "Epoca: 80 Loss: 1.3012477855889057\n",
      "Epoca: 100 Loss: 0.7633017685375088\n",
      "Epoca: 120 Loss: 1.3331253720035872\n",
      "Epoca: 140 Loss: 1.320959683567271\n",
      "Epoca: 160 Loss: 1.308987279226634\n",
      "Epoca: 180 Loss: 1.2972050889458178\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 200\n",
    "TAXA_DE_APRENDIZADO = 0.5\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    # forward pass\n",
    "    y_pred = []\n",
    "    for exemplo in x:\n",
    "        previsao = minha_mlp(exemplo)\n",
    "        y_pred.append(previsao)\n",
    "\n",
    "    # loss\n",
    "    erros = []\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        residuo = yp - yt\n",
    "        erro_quadratico = residuo ** 2\n",
    "        erros.append(erro_quadratico)        \n",
    "    loss = sum(erros)\n",
    "\n",
    "    # zero grad\n",
    "    for p in minha_mlp.parametros():\n",
    "        p.grad = 0\n",
    "\n",
    "    # backpropagation\n",
    "    loss.propagar_tudo()\n",
    "\n",
    "    # atualiza par√¢metros\n",
    "    for p in minha_mlp.parametros():\n",
    "        p.data = p.data - p.grad * TAXA_DE_APRENDIZADO\n",
    "\n",
    "    if epoca % 20 == 0:\n",
    "        print(\"Epoca:\", epoca, \"Loss:\", loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1d6331a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0.2, 0.5]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4421b932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Valor(data=-0.032322874905503715),\n",
       " Valor(data=-0.03228405052139113),\n",
       " Valor(data=-0.032285647576691244),\n",
       " Valor(data=-0.03229056028166969)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d20a91",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Preciso entender como as fun√ß√µes de ativa√ß√£o criadas est√£o dando dados t√£o ruins para essas previ√µes.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cb051",
   "metadata": {},
   "source": [
    "## üìñ Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1c50b5",
   "metadata": {},
   "source": [
    "[1] https://www.v7labs.com/blog/neural-networks-activation-functions\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "[3] https://www.datacamp.com/tutorial/introduction-to-activation-functions-in-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01390853",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
