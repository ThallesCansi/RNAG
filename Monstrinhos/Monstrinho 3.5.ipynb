{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880c8fa6",
   "metadata": {},
   "source": [
    "# üßå Monstrinho 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4a243",
   "metadata": {},
   "source": [
    "O sol amanhece no reino de Lumi e um novo monstro aparece para incomodar. Vamos l√° ent√£o. Para derrotar esse monstro, vamos utilizar 3 novas fun√ß√µes de ativa√ß√£o na rede neural. Realizaremos isso atrav√©s dos novos conceitos aprendidos na aula de MLP (Multi-Layer Perceptron) onde montamos um modelo do zero com Python puro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab436a",
   "metadata": {},
   "source": [
    "A seguir, est√£o as 4 classes (`Valor`, `Neur√¥nio`, `Camada` e `MLP`) j√° criadas na aula de MLP. As mudan√ßas que existem foram feitas por mim para melhorar a legibilidade e a organiza√ß√£o do c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2af54",
   "metadata": {},
   "source": [
    "## üî¢ Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504be265",
   "metadata": {},
   "source": [
    "Esta √© a classe Valor, que representa um valor num√©rico com suporte a diferencia√ß√£o autom√°tica. Ela possui m√©todos para opera√ß√µes matem√°ticas, como adi√ß√£o, subtra√ß√£o, multiplica√ß√£o e exponencia√ß√£o, al√©m de m√©todos para calcular a fun√ß√£o sigmoide e backpropagation. √â not√≥rio lembrar que seja poss√≠vel realizar os c√°lculos mesmo com ordem invertida, onde o objeto Valor pode ser o primeiro ou segundo operando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59390afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Valor:\n",
    "    \"\"\"\n",
    "    Classe que representa um valor num√©rico com suporte a diferencia√ß√£o autom√°tica.\n",
    "    \n",
    "    Cada inst√¢ncia armazena:\n",
    "      - data: o valor num√©rico.\n",
    "      - progenitor: tupla com os valores dos quais este foi derivado.\n",
    "      - operador_mae: string representando a opera√ß√£o que gerou o valor.\n",
    "      - grad: gradiente (inicialmente zero) usado no backpropagation.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, progenitor=(), operador_mae=\"\", rotulo=\"\"):\n",
    "        \"\"\"\n",
    "        Inicializa uma inst√¢ncia de Valor.\n",
    "        \n",
    "        Args:\n",
    "            data (float): o valor num√©rico.\n",
    "            progenitor (tuple): valores anteriores que contribu√≠ram para este.\n",
    "            operador_mae (str): opera√ß√£o que gerou o valor.\n",
    "            rotulo (str): r√≥tulo opcional para identifica√ß√£o.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.progenitor = progenitor\n",
    "        self.operador_mae = operador_mae\n",
    "        self.rotulo = rotulo\n",
    "        self.grad = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Retorna uma representa√ß√£o string simplificada do objeto.\"\"\"\n",
    "        return f\"Valor(data={self.data})\"\n",
    "\n",
    "    def __add__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de adi√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self + outro_valor.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser somado.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a soma.\n",
    "        \"\"\"\n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "            \n",
    "        progenitor = (self, outro_valor)\n",
    "        data = self.data + outro_valor.data\n",
    "        operador_mae = \"+\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_adicao():\n",
    "            self.grad += resultado.grad\n",
    "            outro_valor.grad += resultado.grad\n",
    "            \n",
    "        resultado.propagar = propagar_adicao\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "    def __mul__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de multiplica√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self * outro_valor.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser multiplicado.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando o produto.\n",
    "        \"\"\"\n",
    "        if not isinstance(outro_valor, Valor):\n",
    "            outro_valor = Valor(outro_valor)\n",
    "            \n",
    "        progenitor = (self, outro_valor)\n",
    "        data = self.data * outro_valor.data\n",
    "        operador_mae = \"*\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_multiplicacao():\n",
    "            self.grad += resultado.grad * outro_valor.data\n",
    "            outro_valor.grad += resultado.grad * self.data\n",
    "            \n",
    "        resultado.propagar = propagar_multiplicacao\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Calcula a exponencial do valor.\n",
    "        \n",
    "        Realiza a opera√ß√£o: exp(self).\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a exponencial.\n",
    "        \"\"\"\n",
    "        progenitor = (self, )\n",
    "        data = math.exp(self.data)\n",
    "        operador_mae = \"exp\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_exp():\n",
    "            self.grad += resultado.grad * data \n",
    "        \n",
    "        resultado.propagar = propagar_exp\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "    def __pow__(self, expoente):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de exponencia√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self ** expoente.\n",
    "        \n",
    "        Args:\n",
    "            expoente (int ou float): expoente da opera√ß√£o.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a exponencia√ß√£o.\n",
    "        \"\"\"\n",
    "        assert isinstance(expoente, (int, float)), \"Expoente deve ser um n√∫mero.\"\n",
    "        progenitor = (self, )\n",
    "        data = self.data ** expoente\n",
    "        operador_mae = f\"**{expoente}\"\n",
    "        resultado = Valor(data, progenitor, operador_mae)\n",
    "        \n",
    "        def propagar_pow():\n",
    "            self.grad += resultado.grad * (expoente * self.data ** (expoente - 1))\n",
    "        \n",
    "        resultado.propagar = propagar_pow\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "    def __truediv__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de divis√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self / outro_valor.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): divisor.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a divis√£o.\n",
    "        \"\"\"\n",
    "        return self * outro_valor ** (-1)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de nega√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: -self.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando o valor negativo.\n",
    "        \"\"\"\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de subtra√ß√£o.\n",
    "        \n",
    "        Realiza a opera√ß√£o: self - outro_valor.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser subtra√≠do.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando a subtra√ß√£o.\n",
    "        \"\"\"\n",
    "        return self + (-outro_valor)\n",
    "\n",
    "    def __radd__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de adi√ß√£o reversa.\n",
    "        \n",
    "        Permite opera√ß√µes onde Valor est√° √† direita: outro_valor + self.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser somado.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: resultado da adi√ß√£o.\n",
    "        \"\"\"\n",
    "        return self + outro_valor\n",
    "\n",
    "    def __rmul__(self, outro_valor):\n",
    "        \"\"\"\n",
    "        Sobrecarga do operador de multiplica√ß√£o reversa.\n",
    "        \n",
    "        Permite opera√ß√µes onde Valor est√° √† direita: outro_valor * self.\n",
    "        \n",
    "        Args:\n",
    "            outro_valor (Valor ou n√∫mero): valor a ser multiplicado.\n",
    "        \n",
    "        Returns:\n",
    "            Valor: resultado da multiplica√ß√£o.\n",
    "        \"\"\"\n",
    "        return self * outro_valor\n",
    "\n",
    "    def sig(self):\n",
    "        \"\"\"\n",
    "        Calcula a fun√ß√£o sigmoide.\n",
    "        \n",
    "        Realiza a opera√ß√£o: exp(self) / (exp(self) + 1).\n",
    "        \n",
    "        Returns:\n",
    "            Valor: novo objeto representando o resultado da sigmoide.\n",
    "        \"\"\"\n",
    "        return self.exp() / (self.exp() + 1)\n",
    "\n",
    "    def propagar(self):\n",
    "        \"\"\"\n",
    "        Fun√ß√£o de propaga√ß√£o (backpropagation) do gradiente.\n",
    "        \n",
    "        Este m√©todo deve ser sobrescrito pelas opera√ß√µes espec√≠ficas.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def propagar_tudo(self):\n",
    "        \"\"\"\n",
    "        Executa o backpropagation atrav√©s de todos os n√≥s (valores) conectados.\n",
    "        \n",
    "        Atribui gradiente 1 ao v√©rtice folha e propaga recursivamente utilizando uma ordem topol√≥gica dos n√≥s.\n",
    "        \"\"\"\n",
    "        self.grad = 1\n",
    "        ordem_topologica = []\n",
    "        visitados = set()\n",
    "\n",
    "        def constroi_ordem_topologica(v):\n",
    "            if v not in visitados:\n",
    "                visitados.add(v)\n",
    "                for progenitor in v.progenitor:\n",
    "                    constroi_ordem_topologica(progenitor)\n",
    "                ordem_topologica.append(v)\n",
    "\n",
    "        constroi_ordem_topologica(self)\n",
    "\n",
    "        for vertice in reversed(ordem_topologica):\n",
    "            vertice.propagar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2b97f",
   "metadata": {},
   "source": [
    "## ü§ì Neur√¥nio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd5247",
   "metadata": {},
   "source": [
    "A classe Neur√¥nio representa um neur√¥nio em uma rede neural. Ela possui pesos e um vi√©s, que s√£o inicializados aleatoriamente. O neur√¥nio calcula a sa√≠da usando a fun√ß√£o sigmoide e realiza o backpropagation para atualizar os pesos e o vi√©s com base no erro da previs√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0c222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Neuronio:\n",
    "    \"\"\"\n",
    "    Representa um neur√¥nio simples com pesos e vi√©s para uso em uma rede neural.\n",
    "\n",
    "    Este neur√¥nio utiliza a classe Valor para armazenar seus par√¢metros e realizar a\n",
    "    diferencia√ß√£o autom√°tica durante o treinamento.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_dados_entrada, ativacao=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Inicializa um neur√¥nio com pesos e vi√©s aleat√≥rios.\n",
    "        \n",
    "        Args:\n",
    "            num_dados_entrada (int): n√∫mero de entradas do neur√¥nio.\n",
    "            ativacao (str): fun√ß√£o de ativa√ß√£o a ser utilizada. Op√ß√µes: \"sigmoid\", \"relu\", \"tanh\" e \"leaky_relu\".\n",
    "        \"\"\"\n",
    "        self.vies = Valor(random.uniform(-1, 1))\n",
    "\n",
    "        self.pesos = []\n",
    "        for i in range(num_dados_entrada):\n",
    "            self.pesos.append(Valor(random.uniform(-1, 1)))\n",
    "\n",
    "        if ativacao == \"sigmoid\":\n",
    "            self.ativacao = lambda x: x.sig()\n",
    "        elif ativacao == \"relu\":\n",
    "            self.ativacao = lambda x: x.relu()\n",
    "        elif ativacao == \"tanh\":\n",
    "            self.ativacao = lambda x: x.tanh()\n",
    "        elif ativacao == \"leaky_relu\":\n",
    "            self.ativacao = lambda x: x.leaky_relu()\n",
    "        else:\n",
    "            raise ValueError(f\"Fun√ß√£o de ativa√ß√£o '{ativacao}' n√£o suportada.\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Realiza a passagem forward do neur√¥nio.\n",
    "\n",
    "        Calcula a soma ponderada das entradas e aplica a fun√ß√£o sigmoide para\n",
    "        determinar a sa√≠da do neur√¥nio.\n",
    "\n",
    "        Args:\n",
    "            x (list[Valor]): lista de objetos Valor representando as entradas.\n",
    "\n",
    "        Returns:\n",
    "            Valor: objeto Valor representando a sa√≠da do neur√¥nio.\n",
    "        \"\"\"\n",
    "        assert len(x) == len(\n",
    "            self.pesos\n",
    "        ), \"O n√∫mero de entradas deve ser igual ao n√∫mero de pesos.\"\n",
    "\n",
    "        soma = 0\n",
    "        for info_entrada, peso_interno in zip(x, self.pesos):\n",
    "            soma += info_entrada * peso_interno\n",
    "\n",
    "        soma += self.vies\n",
    "\n",
    "        return self.ativacao(soma)\n",
    "\n",
    "    def parametros(self):\n",
    "        \"\"\"\n",
    "        Retorna uma lista com os par√¢metros do neur√¥nio (pesos e vi√©s).\n",
    "\n",
    "        Returns:\n",
    "            list[Valor]: lista contendo os pesos e o vi√©s.\n",
    "        \"\"\"\n",
    "        return self.pesos + [self.vies]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62566653",
   "metadata": {},
   "source": [
    "Uma pequena altera√ß√£o ocorreu na classe `Neuronio` para permitir a utiliza√ß√£o de fun√ß√µes de ativa√ß√£o diferentes. Para isso, foi adicionado um par√¢metro `ativacao` no construtor da classe. Esse par√¢metro pode ser uma fun√ß√£o de ativa√ß√£o como `sigmoide`, `relu`, `tanh` ou `leaky_relu`. O m√©todo `ativar` agora utiliza essa fun√ß√£o de ativa√ß√£o para calcular a sa√≠da do neur√¥nio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab5730",
   "metadata": {},
   "source": [
    "## üéÇ Camada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b70f2",
   "metadata": {},
   "source": [
    "A classe Camada representa uma camada de neur√¥nios em uma rede neural. Ela possui um n√∫mero espec√≠fico de neur√¥nios e √© respons√°vel por calcular a sa√≠da da camada com base nas entradas recebidas. Realiza o forward pass e agrega os parametros de cada neur√¥nio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7f3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camada:\n",
    "    \"\"\"\n",
    "    Representa uma camada em uma rede neural composta por m√∫ltiplos neur√¥nios.\n",
    "\n",
    "    Cada camada gerencia um conjunto de neur√¥nios, realizando a passagem forward\n",
    "    e agregando os par√¢metros (pesos e vi√©s) de cada neur√¥nio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_neuronios, num_dados_entrada, ativacao=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Inicializa a camada com um n√∫mero espec√≠fico de neur√¥nios, cada um com\n",
    "        um determinado n√∫mero de entradas.\n",
    "\n",
    "        Args:\n",
    "            num_neuronios (int): n√∫mero de neur√¥nios na camada.\n",
    "            num_dados_entrada (int): n√∫mero de entradas para cada neur√¥nio.\n",
    "        \"\"\"\n",
    "        self.neuronios = []\n",
    "        for _ in range(num_neuronios):\n",
    "            neuronio = Neuronio(num_dados_entrada, ativacao=ativacao)\n",
    "            self.neuronios.append(neuronio)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Realiza a passagem forward na camada.\n",
    "\n",
    "        Aplica cada neur√¥nio da camada √† mesma entrada e retorna os dados de sa√≠da.\n",
    "\n",
    "        Args:\n",
    "            x (list[Valor]): lista de objetos Valor representando as entradas da camada.\n",
    "\n",
    "        Returns:\n",
    "            Valor ou list[Valor]: sa√≠da de um √∫nico neur√¥nio se houver apenas um,\n",
    "            ou lista com as sa√≠das de todos os neur√¥nios.\n",
    "        \"\"\"\n",
    "        dados_de_saida = []\n",
    "        for neuronio in self.neuronios:\n",
    "            informacao = neuronio(x)\n",
    "            dados_de_saida.append(informacao)\n",
    "\n",
    "        if len(dados_de_saida) == 1:\n",
    "            return dados_de_saida[0]\n",
    "        else:\n",
    "            return dados_de_saida\n",
    "\n",
    "    def parametros(self):\n",
    "        \"\"\"\n",
    "        Agrega e retorna todos os par√¢metros (pesos e vi√©s) de cada neur√¥nio da camada.\n",
    "\n",
    "        Returns:\n",
    "            list[Valor]: lista contendo todos os par√¢metros dos neur√¥nios da camada.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for neuronio in self.neuronios:\n",
    "            params_neuronio = neuronio.parametros()\n",
    "            params.extend(params_neuronio)\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab290f7",
   "metadata": {},
   "source": [
    "A classe `Camada` tamb√©m foi alterada para permitir a utiliza√ß√£o de fun√ß√µes de ativa√ß√£o diferentes. O construtor agora aceita um par√¢metro `ativacao` que pode ser uma fun√ß√£o de ativa√ß√£o como `sigmoide`, `relu`, `tanh` ou `leaky_relu`. O m√©todo `ativar` utiliza essa fun√ß√£o de ativa√ß√£o para calcular a sa√≠da da camada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1119298",
   "metadata": {},
   "source": [
    "## üß† MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcbbe9",
   "metadata": {},
   "source": [
    "Por fim, nossa √∫ltima classe da nossa rede neural √© a `MLP` (Multi-Layer Perceptron). Ela representa uma rede neural com m√∫ltiplas camadas. A `MLP` organiza as camadas da rede, permitindo a passagem forward dos dados e a agrega√ß√£o dos par√¢metros de todas as camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a4989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Representa uma rede neural do tipo MLP (Multi-Layer Perceptron).\n",
    "\n",
    "    Essa classe organiza as camadas da rede, permitindo a passagem forward dos dados e\n",
    "    a agrega√ß√£o dos par√¢metros (pesos e vi√©s) de todas as camadas.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_dados_entrada, num_neuronios_por_camada, ativacao=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Inicializa a MLP com um n√∫mero definido de entradas e uma lista que especifica\n",
    "        o n√∫mero de neur√¥nios em cada camada.\n",
    "\n",
    "        Args:\n",
    "            num_dados_entrada (int): n√∫mero de entradas da rede.\n",
    "            num_neuronios_por_camada (list[int]): lista com o n√∫mero de neur√¥nios para cada camada.\n",
    "        \"\"\"\n",
    "        percurso = [num_dados_entrada] + num_neuronios_por_camada\n",
    "\n",
    "        camadas = []\n",
    "        for i in range(len(num_neuronios_por_camada)):\n",
    "            camada = Camada(num_neuronios_por_camada[i], percurso[i], ativacao=ativacao)\n",
    "            camadas.append(camada)\n",
    "\n",
    "        self.camadas = camadas\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Realiza a passagem forward pela rede.\n",
    "\n",
    "        Cada camada processa a entrada e o resultado √© passado para a pr√≥xima camada.\n",
    "\n",
    "        Args:\n",
    "            x (list[Valor] ou Valor): dados de entrada para a rede.\n",
    "\n",
    "        Returns:\n",
    "            Valor ou list[Valor]: sa√≠da final da rede ap√≥s a passagem por todas as camadas.\n",
    "        \"\"\"\n",
    "        for camada in self.camadas:\n",
    "            x = camada(x)\n",
    "        return x\n",
    "\n",
    "    def parametros(self):\n",
    "        \"\"\"\n",
    "        Agrega e retorna todos os par√¢metros (pesos e vi√©s) de todas as camadas da rede.\n",
    "\n",
    "        Returns:\n",
    "            list[Valor]: lista contendo os par√¢metros de cada camada.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for camada in self.camadas:\n",
    "            parametros_camada = camada.parametros()\n",
    "            params.extend(parametros_camada)\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b3795",
   "metadata": {},
   "source": [
    "Por fim, a classe `MLP` tamb√©m foi alterada para permitir a utiliza√ß√£o de fun√ß√µes de ativa√ß√£o diferentes. O construtor agora aceita um par√¢metro `ativacao` que pode ser uma fun√ß√£o de ativa√ß√£o como `sigmoide`, `relu`, `tanh` ou `leaky_relu`. O m√©todo `ativar` utiliza essa fun√ß√£o de ativa√ß√£o para calcular a sa√≠da da rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68c58c",
   "metadata": {},
   "source": [
    "## ü™ù Fun√ß√µes de Ativa√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8bc988",
   "metadata": {},
   "source": [
    "Show! Agora que definimos todas as classes que usaremos para essa atividade, vamos abordar as 3 novas fun√ß√µes de ativa√ß√£o que vamos usar para treinar nossa rede neural. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2ff3e",
   "metadata": {},
   "source": [
    "### 0Ô∏è‚É£ Sigmoide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec8938",
   "metadata": {},
   "source": [
    "Antes de mostrarmos as novas fun√ß√µes de ativa√ß√£o, vamos relembrar a fun√ß√£o sigmoide. A fun√ß√£o sigmoide √© uma fun√ß√£o matem√°tica que transforma um valor real em um valor entre 0 e 1. Ela √© definida pela f√≥rmula:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Sua principal caracter√≠stica √© que ela \"achata\" os valores extremos, ou seja, valores muito grandes ou muito pequenos s√£o aproximados para 1 ou 0, respectivamente. Isso pode ser √∫til em redes neurais, pois ajuda a evitar problemas de satura√ß√£o e facilita o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa290fc4",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/Sigmoid.jpg\" alt=\"ReLU Function\" width=\"300\"/>\n",
    "<p>Legenda 1: Esta √© a fun√ß√£o Sigmoide. Ela transforma valores reais em valores entre 0 e 1.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69069d14",
   "metadata": {},
   "source": [
    "A fun√ß√£o sigmoide √© amplamente utilizada em redes neurais, especialmente em problemas de classifica√ß√£o bin√°ria. Ela √© uma fun√ß√£o suave e diferenci√°vel, o que a torna adequada para otimiza√ß√£o usando algoritmos de aprendizado de m√°quina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0f738",
   "metadata": {},
   "source": [
    "A fun√ß√£o sigmoide j√° foi implementada na classe `Valor` e √© utilizada como fun√ß√£o de ativa√ß√£o padr√£o nos neur√¥nios. Vamos citar a fun√ß√£o sigmoide novamente aqui para relembrar como ela √© implementada: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082e5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(self):\n",
    "    \"\"\"\n",
    "    Calcula a fun√ß√£o sigmoide.\n",
    "\n",
    "    Realiza a opera√ß√£o: exp(self) / (exp(self) + 1).\n",
    "\n",
    "    Returns:\n",
    "        Valor: novo objeto representando o resultado da sigmoide.\n",
    "    \"\"\"\n",
    "    return self.exp() / (self.exp() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a76a50",
   "metadata": {},
   "source": [
    "> Voc√™ pode notar que existe uma pequena diferen√ßa entre a f√≥rmula utilizada na fun√ß√£o (definida pelo professor) e pela apresentada acima. Mas, √© a mesma coisa, s√£o equivalentes. Aqui est√° a explica√ß√£o matem√°tica para isso:\n",
    "> $$\n",
    "> f(x) = \\frac{1}{1 + e^{-x}} \\times \\frac{e^{x}}{e^{x}} = \\frac{e^{x}}{e^{x} + 1}\n",
    "> $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9fc78",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127eec73",
   "metadata": {},
   "source": [
    "A ReLU √© uma das fun√ß√µes de ativa√ß√£o mais populares para redes neurais artificiais, e encontra aplica√ß√£o em vis√£o computacional e reconhecimento de fala usando redes neurais profundas e neuroci√™ncia computacional. [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ca5f5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fd7b3",
   "metadata": {},
   "source": [
    "Uma das principais caracter√≠sticas da fun√ß√£o de ativa√ß√£o ReLU s√£o:\n",
    "\n",
    "- Simples e eficiente\n",
    "- Resolve parcialmente o problema do **desvanecimento do gradiente**.\n",
    "- Pode \"morrer\" para entradas negativas (gradiente zero).\n",
    "\n",
    "Quando dizemos que a ReLU √© simples e eficiente, nos referimos ao fato de que ela √© computacionalmente barata, pois envolve apenas uma compara√ß√£o e n√£o requer opera√ß√µes exponenciais ou trigonom√©tricas. Isso a torna adequada para redes neurais profundas, onde a efici√™ncia computacional √© crucial. J√° o problema do desvanecimento do gradiente ocorre quando os gradientes se tornam muito pequenos durante o treinamento, dificultando a atualiza√ß√£o dos pesos. A ReLU ajuda a mitigar esse problema, permitindo que os gradientes fluam mais facilmente atrav√©s das camadas da rede. No entanto, a ReLU pode \"morrer\" para entradas negativas, resultando em neur√¥nios que n√£o se ativam e n√£o contribuem para o aprendizado. Isso pode ocorrer quando os pesos s√£o atualizados de forma que a sa√≠da do neur√¥nio permane√ßa negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cfef0",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/ReLU.jpg\" alt=\"ReLU Function\" width=\"300\"/>\n",
    "<p>Legenda 2: Esta √© a fun√ß√£o ReLU. Ela √© linear para valores positivos e zero para valores negativos.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c54ce",
   "metadata": {},
   "source": [
    "Comparando com a fun√ß√£o de ativa√ß√£o sigmoide que mapeia para o intervalo (0, 1), o que pode gerar gradientes pequenos em extremos. Mas, a ReLU j√° √© mais eficiente e n√£o satura no lado positivo, o que significa que os gradientes permanecem grandes e n√£o se tornam pequenos, permitindo que a rede aprenda mais rapidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58acfa2f",
   "metadata": {},
   "source": [
    "Vamos aproveitar, e j√° criar a fun√ß√£o em Python para a ReLU. A fun√ß√£o `relu` recebe um valor e retorna o valor m√°ximo entre 0 e o valor de entrada. Se o valor for menor que 0, a fun√ß√£o retorna 0. Caso contr√°rio, retorna o pr√≥prio valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36703c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(self: Valor) -> Valor:\n",
    "    \"\"\"\n",
    "    Aplica a fun√ß√£o ReLU (Rectified Linear Unit) ao valor. A fun√ß√£o ReLU retorna o valor se ele for positivo, caso contr√°rio retorna 0. Realiza a opera√ß√£o: max(0, self).\n",
    "    \n",
    "    Args:\n",
    "        Valor (self): valor a ser processado.\n",
    "        \n",
    "    Returns:\n",
    "        Valor: novo objeto representando o resultado da fun√ß√£o ReLU.\n",
    "    \"\"\"\n",
    "    x = self\n",
    "    data = x.data if x.data > 0 else 0.0\n",
    "    resultado = Valor(data, (x,), \"ReLU\")\n",
    "\n",
    "    def propagar_relu():\n",
    "        x.grad += resultado.grad * (1.0 if x.data > 0 else 0.0)\n",
    "    \n",
    "    resultado.propagar = propagar_relu\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3eb49d",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Tanh (Tangente Hiperb√≥lica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1106c",
   "metadata": {},
   "source": [
    "A fun√ß√£o tanh produz valores no intervalo de -1 a +1. Isso significa que ele pode lidar com valores negativos de forma mais eficaz do que a fun√ß√£o sigm√≥ide, que tem um intervalo de 0 a 1. [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f9de0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tanh(x) = \\frac{(e^x - e^{-x})}{(e^x + e^{-x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9de24",
   "metadata": {},
   "source": [
    "As principais caracter√≠sticas que podemos destacar da fun√ß√£o tanh s√£o:\n",
    "\n",
    "- Sa√≠da entre (-1, 1), centrada em zero.\n",
    "- Derivada maior que a da sigmoide, o que acelera o aprendizado.\n",
    "\n",
    "A sa√≠da entre (-1, 1) significa que a fun√ß√£o tanh pode lidar com valores negativos de forma mais eficaz do que a fun√ß√£o sigmoide, que tem um intervalo de 0 a 1. Isso pode ser √∫til em redes neurais profundas, onde a normaliza√ß√£o dos dados pode ser importante. A derivada da fun√ß√£o tanh √© maior do que a da sigmoide, o que significa que os gradientes s√£o mais fortes e podem acelerar o aprendizado. Isso pode levar a uma converg√™ncia mais r√°pida durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75d391",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/Tanh.jpg\" alt=\"Tanh Function\" width=\"300\"/>\n",
    "<p>Legenda 3: Esta j√° √© a fun√ß√£o tanh. Ela √© linear para valores pr√≥ximos de zero e n√£o linear para valores extremos.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28acad",
   "metadata": {},
   "source": [
    "Comparando com a fun√ß√£o sigmoide, a tanh √© mais adequada para entradas centradas em zero. Isso significa que a fun√ß√£o tanh pode lidar melhor com dados que t√™m uma m√©dia pr√≥xima de zero, o que pode ser √∫til em muitas aplica√ß√µes de aprendizado de m√°quina. Entretanto, a tanh ainda sofre com o problema do desvanecimento de gradiente em extremos, onde os gradientes se tornam muito pequenos e dificultam o aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408bfb2",
   "metadata": {},
   "source": [
    "Bom, agora vamos criar a fun√ß√£o em Python para a tanh. A fun√ß√£o `tanh` recebe um valor e retorna o valor da tangente hiperb√≥lica do valor de entrada. Se o valor for menor que 0, a fun√ß√£o retorna -1. Caso contr√°rio, retorna o pr√≥prio valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b5fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(self: Valor) -> Valor:\n",
    "    \"\"\"\n",
    "    Aplica a fun√ß√£o tangente hiperb√≥lica (tanh) ao valor. A fun√ß√£o tanh retorna o valor normalizado entre -1 e 1. Realiza a opera√ß√£o: tanh(self).\n",
    "\n",
    "    Args:\n",
    "        Valor (self): valor a ser processado.\n",
    "\n",
    "    Returns:\n",
    "        Valor: novo objeto representando o resultado da fun√ß√£o tanh.\n",
    "    \"\"\"\n",
    "    x = self\n",
    "    e_pos = (x * 2).exp()\n",
    "    t = (e_pos - 1) / (e_pos + 1)\n",
    "    resultado = Valor(t.data, (x,), \"tanh\")\n",
    "\n",
    "    def propagar_tanh():\n",
    "        x.grad += resultado.grad * (1 - t.data**2)\n",
    "\n",
    "    resultado.propagar = propagar_tanh\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ff120",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfade8",
   "metadata": {},
   "source": [
    "Leaky ReLU √© uma vers√£o melhorada da fun√ß√£o ReLU para resolver o problema de \"morte\" da ReLU, pois tem uma pequena inclina√ß√£o positiva na √°rea negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074cddd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Leaky ReLU}(x) = \n",
    "\\begin{cases}\n",
    "x & \\text{se } x > 0 \\\\\n",
    "\\alpha x & \\text{caso contr√°rio}\n",
    "\\end{cases}\n",
    "\\quad \\text{(tipicamente } \\alpha = 0.01\\text{)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94440b8a",
   "metadata": {},
   "source": [
    "As principais caracter√≠sticas que podemos destacar da fun√ß√£o Leaky ReLU s√£o:\n",
    "\n",
    "- Variante do ReLU que permite gradiente pequeno quando $ x < 0 $.\n",
    "- Reduz o risco de \"neur√¥nios mortos\".\n",
    "\n",
    "A possibilidade de permitir um pequeno gradiente negativo quando $ x < 0 $ acontece devido √† inclina√ß√£o pequena, o que significa que mesmo quando a entrada √© negativa, a fun√ß√£o ainda tem um pequeno gradiente positivo. Isso ajuda a evitar o problema de \"neur√¥nios mortos\", onde os neur√¥nios n√£o se ativam e n√£o contribuem para o aprendizado. A Leaky ReLU √© uma fun√ß√£o de ativa√ß√£o mais robusta do que a ReLU padr√£o, pois pode lidar melhor com entradas negativas e ainda mant√©m as vantagens da ReLU em termos de efici√™ncia computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd22da",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/Leaky ReLU.jpg\" alt=\"Tanh Function\" width=\"300\"/>\n",
    "<p>Legenda 4: Por fim, temos a Leaky ReLU. Ela √© linear para valores positivos e tem uma pequena inclina√ß√£o negativa para valores negativos.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bb9db",
   "metadata": {},
   "source": [
    "Realizando uma compara√ß√£o com a fun√ß√£o sigmoide, a Leaky ReLU √© mais eficiente em termos computacionais, pois n√£o envolve opera√ß√µes exponenciais ou trigonom√©tricas. Isso a torna adequada para redes neurais profundas, onde a efici√™ncia computacional √© crucial. Al√©m disso, a Leaky ReLU n√£o sofre com o problema do desvanecimento do gradiente da mesma forma que a sigmoide e a tanh, permitindo que os gradientes fluam mais facilmente atrav√©s das camadas da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456428b",
   "metadata": {},
   "source": [
    "Para terminar, vamos criar a fun√ß√£o em Python para a Leaky ReLU. A fun√ß√£o `leaky_relu` recebe um valor e retorna o valor m√°ximo entre 0 e o valor de entrada. Se o valor for menor que 0, a fun√ß√£o retorna 0.01 vezes o valor de entrada. Caso contr√°rio, retorna o pr√≥prio valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aaab39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(self, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Aplica a fun√ß√£o Leaky ReLU ao valor. A fun√ß√£o Leaky ReLU retorna o valor se ele for positivo, caso contr√°rio retorna alpha * valor. Realiza a opera√ß√£o: max(0, self) + alpha * min(0, self).\n",
    "\n",
    "    Args:\n",
    "        Valor (self): valor a ser processado.\n",
    "        alpha (float): coeficiente de inclina√ß√£o para valores negativos.\n",
    "\n",
    "    Returns:\n",
    "        Valor: novo objeto representando o resultado da fun√ß√£o Leaky ReLU.\n",
    "    \"\"\"\n",
    "    x = self\n",
    "    data = x.data if x.data > 0 else alpha * x.data\n",
    "    resultado = Valor(data, (x,), \"LeakyReLU\")\n",
    "\n",
    "    def propagar_leaky():\n",
    "        x.grad += resultado.grad * (1.0 if x.data > 0 else alpha)\n",
    "\n",
    "    resultado.propagar = propagar_leaky\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb0dce",
   "metadata": {},
   "source": [
    "### üß™ Testando as fun√ß√µes de ativa√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776a123",
   "metadata": {},
   "source": [
    "Primeiro, para testarmos essas fun√ß√µes de ativa√ß√£o, precisamos instanciar a classe `Valor` e criar um valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a898c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valor.relu = relu\n",
    "Valor.tanh = tanh\n",
    "Valor.leaky_relu = leaky_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c64f3",
   "metadata": {},
   "source": [
    "Agora que definimos na classe `Valor` as fun√ß√µes de ativa√ß√£o que definimos anteriormente, vamos utiliz√°-las com o mesmo exemplo que o professor usou na aula de MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a45f2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_mlp(\n",
    "    x,\n",
    "    y_true,\n",
    "    arquitetura,\n",
    "    ativacao=\"sigmoid\",\n",
    "    num_epocas=200,\n",
    "    taxa_de_aprendizado=0.01,\n",
    "    verbose_interval=50,\n",
    "):\n",
    "    n_in = len(x[0])\n",
    "    rede = MLP(n_in, arquitetura, ativacao=ativacao)\n",
    "\n",
    "    loss_hist = []\n",
    "\n",
    "    for ep in range(num_epocas):\n",
    "        y_pred = [rede([Valor(v) for v in amostra]) for amostra in x]\n",
    "\n",
    "        erros = [(yp - yt) ** 2 for yt, yp in zip(y_true, y_pred)]\n",
    "        loss = sum(erros) / len(erros)\n",
    "\n",
    "        for p in rede.parametros():\n",
    "            p.grad = 0\n",
    "\n",
    "        loss.propagar_tudo()\n",
    "\n",
    "        for p in rede.parametros():\n",
    "            p.data -= taxa_de_aprendizado * p.grad\n",
    "\n",
    "        loss_hist.append(loss.data)\n",
    "        if verbose_interval and ep % verbose_interval == 0:\n",
    "            print(f\"[{ativacao}] √©poca {ep:4d}  loss={loss.data:.6f}\")\n",
    "\n",
    "    preds = [rede([Valor(v) for v in amostra]).data for amostra in x]\n",
    "    return rede, preds, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b11f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SIGMOID ===\n",
      "[sigmoid] √©poca    0  loss=0.176704\n",
      "[sigmoid] √©poca  200  loss=0.132572\n",
      "[sigmoid] √©poca  400  loss=0.062238\n",
      "[sigmoid] √©poca  600  loss=0.017479\n",
      "[sigmoid] √©poca  800  loss=0.007029\n",
      "\n",
      "=== TANH ===\n",
      "[tanh] √©poca    0  loss=0.265223\n",
      "[tanh] √©poca  400  loss=0.010209\n",
      "[tanh] √©poca  800  loss=0.002614\n",
      "[tanh] √©poca 1200  loss=0.001254\n",
      "[tanh] √©poca 1600  loss=0.000788\n",
      "\n",
      "=== RELU ===\n",
      "[relu] √©poca    0  loss=0.322500\n",
      "[relu] √©poca  400  loss=0.322500\n",
      "[relu] √©poca  800  loss=0.322500\n",
      "[relu] √©poca 1200  loss=0.322500\n",
      "[relu] √©poca 1600  loss=0.322500\n",
      "\n",
      "=== LEAKY_RELU ===\n",
      "[leaky_relu] √©poca    0  loss=3.551047\n",
      "[leaky_relu] √©poca  400  loss=0.006452\n",
      "[leaky_relu] √©poca  800  loss=0.000356\n",
      "[leaky_relu] √©poca 1200  loss=0.000095\n",
      "[leaky_relu] √©poca 1600  loss=0.000027\n",
      "\n",
      "Predi√ß√µes finais:\n",
      "   sigmoid: ['0.9021', '0.0780', '0.2003', '0.5089']\n",
      "      tanh: ['0.9526', '-0.0004', '0.2014', '0.5027']\n",
      "      relu: ['0.0000', '0.0000', '0.0000', '0.0000']\n",
      "leaky_relu: ['0.9984', '-0.0010', '0.1981', '0.5048']\n"
     ]
    }
   ],
   "source": [
    "x = [\n",
    "    [2.0,  3.0, -1.0],\n",
    "    [3.0, -1.0,  0.5],\n",
    "    [0.5,  1.0,  1.0],\n",
    "    [1.0,  1.0, -1.0],\n",
    "]\n",
    "y_true = [1, 0, 0.2, 0.5]\n",
    "\n",
    "arquitetura = [3, 2, 1]\n",
    "\n",
    "configs = {\n",
    "    \"sigmoid\":     dict(num_epocas=1000, taxa_de_aprendizado=0.5),\n",
    "    \"tanh\":        dict(num_epocas=2000, taxa_de_aprendizado=0.05),\n",
    "    \"relu\":        dict(num_epocas=2000, taxa_de_aprendizado=0.05),\n",
    "    \"leaky_relu\":  dict(num_epocas=2000, taxa_de_aprendizado=0.01),\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "for ativ, cfg in configs.items():\n",
    "    print(f\"\\n=== {ativ.upper()} ===\")\n",
    "    _, preds, hist = treinar_mlp(\n",
    "        x, y_true,\n",
    "        arquitetura=arquitetura,\n",
    "        ativacao=ativ,\n",
    "        **cfg,\n",
    "        verbose_interval=cfg[\"num_epocas\"]//5\n",
    "    )\n",
    "    resultados[ativ] = preds\n",
    "\n",
    "print(\"\\nPredi√ß√µes finais:\")\n",
    "for ativ, preds in resultados.items():\n",
    "    print(f\"{ativ:>10}: {['%.4f' % p for p in preds]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e816c2",
   "metadata": {},
   "source": [
    "| Ativa√ß√£o                  | Faixa de sa√≠da         | Gradiente t√≠pico                                 | Resultado observado                                                            | Motivo principal                                                                                                                                                                                                                    |\n",
    "| ------------------------- | ---------------------- | ------------------------------------------------ | ------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Sigmoide**              | 0‚ÄØ‚Äì‚ÄØ1                  | gradiente moderado em torno de 0                 | *Loss* caiu de‚ÄØ0.14‚ÄØ‚Üí‚ÄØ0.005; predi√ß√µes ‚âà‚ÄØ\\[0.907,‚ÄØ0.046,‚ÄØ0.205,‚ÄØ0.516]         | Faixa da sigmoide coincide exatamente com os r√≥tulos‚Äëalvo ‚áí rede s√≥ precisou aprender escala fina; taxa de aprendizado alta (0‚ÄØ.5) ajudou a convergir r√°pido.                                                                                        |\n",
    "| **tanh**                  | ‚Äì1‚ÄØ‚Äì‚ÄØ1 (centrada em‚ÄØ0) | maior que sigmoide perto da origem; satura em ¬±1 | *Loss* de‚ÄØ0.21‚ÄØ‚Üí‚ÄØ0.001; predi√ß√µes ‚âà‚ÄØ\\[0.950,‚ÄØ‚Äì0.004,‚ÄØ0.203,‚ÄØ0.505]             | Como a tanh devolve valores negativos, a rede acabou empurrando a soma final ligeiramente positiva para tr√™s amostras e ‚âà‚ÄØ0 para a segunda; precisou de mais √©pocas (e taxa de aprendizado 0.05) mas convergiu bem.                                  |\n",
    "| **ReLU**                  | 0‚ÄØ‚Äì‚ÄØ‚àû (corta <‚ÄØ0)      | 1 quando soma‚ÄØ>‚ÄØ0; **0** quando soma‚ÄØ‚â§‚ÄØ0         | *Loss* permaneceu em 0.3225; sa√≠da 0 para todas as amostras                    | Todas as somas ponderadas do neur√¥nio de sa√≠da come√ßaram negativas ‚Üí gradiente 0 ‚Üí pesos/vieses dessa camada nunca mudaram (‚Äúneur√¥nio morto‚Äù).                                                                                      |\n",
    "| **Leaky‚ÄØReLU** (Œ±‚ÄØ=‚ÄØ0.01) | Œ±x‚ÄØ‚Äì‚ÄØ‚àû                 | Œ± (0.01) quando soma‚ÄØ‚â§‚ÄØ0; 1 quando >‚ÄØ0           | *Loss* despencou de 0.325 ‚Üí 0.010; predi√ß√µes ‚âà‚ÄØ\\[0.999,‚ÄØ‚Äì0.004,‚ÄØ‚Äì0.000,‚ÄØ0.501] | O gradiente pequeno (0.01) salvou o fluxo de erro, mas a sa√≠da negativa persiste para as amostras onde a soma final ficou ‚â§‚ÄØ0; sem uma fun√ß√£o de sa√≠da que comprima/shift, a rede n√£o ‚Äúsabe‚Äù que valores negativos s√£o inadequados. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cb051",
   "metadata": {},
   "source": [
    "## üìñ Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1c50b5",
   "metadata": {},
   "source": [
    "[1] https://www.v7labs.com/blog/neural-networks-activation-functions\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "[3] https://www.datacamp.com/tutorial/introduction-to-activation-functions-in-neural-networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
